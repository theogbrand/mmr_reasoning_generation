{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceM4/RAVEN\", \"center_single\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['panels', 'choices', 'structure', 'meta_matrix', 'meta_target', 'meta_structure', 'target', 'id', 'metadata'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['panels', 'choices', 'structure', 'meta_matrix', 'meta_target', 'meta_structure', 'target', 'id', 'metadata']\n",
      "\n",
      "Panels shape: 8\n",
      "Choices shape: 8\n",
      "Target: 4\n",
      "ID: 3023\n",
      "\n",
      "First panel type: <class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "First choice type: <class 'PIL.PngImagePlugin.PngImageFile'>\n"
     ]
    }
   ],
   "source": [
    "# Examine the structure of a single example\n",
    "example = ds[0]\n",
    "print(\"Keys:\", list(example.keys()))\n",
    "print(\"\\nPanels shape:\", len(example['panels']))\n",
    "print(\"Choices shape:\", len(example['choices']))\n",
    "print(\"Target:\", example['target'])\n",
    "print(\"ID:\", example['id'])\n",
    "\n",
    "# Look at the first panel and choice to understand the image format\n",
    "print(\"\\nFirst panel type:\", type(example['panels'][0]))\n",
    "print(\"First choice type:\", type(example['choices'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAVENRunner import...\n",
      "RAVENRunner class imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test the RAVENRunner class\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from preprocessing.load_raven import RAVENRunner\n",
    "import os\n",
    "\n",
    "# Check if we can import and initialize (without real credentials)\n",
    "print(\"Testing RAVENRunner import...\")\n",
    "print(\"RAVENRunner class imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAVENRunner initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAVENRunner initialized successfully!\n",
      "✓ Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n",
      "✓ Validation sample keys: ['panels', 'choices', 'structure', 'meta_matrix', 'meta_target', 'meta_structure', 'target', 'id', 'metadata']\n",
      "✓ Target: 4 (zero-indexed), ID: 5047\n",
      "✓ Target as 1-indexed choice: 5\n"
     ]
    }
   ],
   "source": [
    "# Test RAVENRunner initialization (won't actually call API)\n",
    "print(\"Testing RAVENRunner initialization...\")\n",
    "\n",
    "try:\n",
    "    # This will load the datasets but won't make API calls\n",
    "    runner = RAVENRunner(\n",
    "        model_name=\"azure/gpt-4.1\",\n",
    "        reasoning_effort=\"high\"\n",
    "    )\n",
    "    print(\"✓ RAVENRunner initialized successfully!\")\n",
    "    print(f\"✓ Loaded datasets: {[(k, len(v)) for k, v in runner.datasets.items()]}\")\n",
    "    \n",
    "    # Test dataset access\n",
    "    val_sample = runner.datasets['validation'][0]\n",
    "    print(f\"✓ Validation sample keys: {list(val_sample.keys())}\")\n",
    "    print(f\"✓ Target: {val_sample['target']} (zero-indexed), ID: {val_sample['id']}\")\n",
    "    print(f\"✓ Target as 1-indexed choice: {val_sample['target'] + 1}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s a mix of Paris’s “must-sees,” plus a few off-the-beaten-track picks. Feel free to mix and match based on your interests and how many days you have:\n",
      "\n",
      "1. Iconic Landmarks  \n",
      " • Eiffel Tower (consider booking a summit-access slot in advance)  \n",
      " • Arc de Triomphe & Champs-Élysées (climb to the top of the arch for great views)  \n",
      " • Notre-Dame Cathedral & Île de la Cité (though still under partial restoration, the exterior and nearby Sainte-Chapelle are stunning)  \n",
      " • Sacré-Cœur Basilica in Montmartre (sunset over Paris)\n",
      "\n",
      "2. World-Class Museums  \n",
      " • Louvre (buy timed-entry tickets; don’t miss the Denon wing—“Mona Lisa,” “Winged Victory”)  \n",
      " • Musée d’Orsay (impressionist masterpieces in a converted railway station)  \n",
      " • Centre Pompidou (modern/contemporary art + its twisting exterior escalator)  \n",
      " • Musée de l’Orangerie (Monet’s Water Lilies in the Tuileries Gardens)\n",
      "\n",
      "3. Charming Neighborhoods  \n",
      " • Le Marais (medieval streets, Place des Vosges, trendy boutiques & falafel stands)  \n",
      " • Latin Quarter / Saint-Michel (student vibes, bookshops, the Panthéon, narrow lanes)  \n",
      " • Canal Saint-Martin (picnic by the water, hip cafés, weekend flea market)  \n",
      " • Montmartre (artists’ square at Place du Tertre, Moulin Rouge, hidden vineyard)\n",
      "\n",
      "4. Green Spaces & Views  \n",
      " • Luxembourg Gardens (formal lawns, Medici fountain, perfect for a picnic)  \n",
      " • Parc des Buttes-Chaumont (cliffs, suspension bridge, fewer tourists)  \n",
      " • Promenade Plantée / Coulée Verte (elevated garden path—precursor to NYC’s High Line)  \n",
      " • Seine riverbanks (especially the Left Bank; consider a Bateaux Mouches or Vedettes du Pont-Neuf cruise)\n",
      "\n",
      "5. Day-Trip Ideas  \n",
      " • Palace of Versailles (the Hall of Mirrors, Trianon palaces & grand gardens)  \n",
      " • Giverny (Monet’s house & water-lily pond—best in spring/summer)  \n",
      " • Champagne region (Reims or Épernay, vineyard tours & tastings)  \n",
      " • Château de Fontainebleau (royal residence, easier to visit than Versailles)\n",
      "\n",
      "6. Culinary & Market Stops  \n",
      " • Rue Cler or Rue Montorgueil (market streets for cheese, bread, wine)  \n",
      " • Marché Bastille or Marché d’Aligre (fresh produce, antiques)  \n",
      " • Classic cafés: Café de Flore, Les Deux Magots (Saint-Germain)  \n",
      " • Pastry musts: croissants at Du Pain et des Idées, macarons at Ladurée or Pierre Hermé, ice cream at Berthillon (Île Saint-Louis)\n",
      "\n",
      "7. Hidden Gems & Fun Extras  \n",
      " • The covered passages (Galerie Vivienne, Passage des Panoramas)  \n",
      " • Musée Jacquemart-André (elegant private collection, quieter than the big museums)  \n",
      " • The Paris Catacombs (book ahead; not for the claustrophobic)  \n",
      " • Rooftop bars for skyline views: Le Perchoir, Terrass’’ Hotel (Montmartre)\n",
      "\n",
      "Sample 3-Day Itinerary  \n",
      "Day 1: Eiffel Tower → Seine cruise → Louvre → Tuileries Garden  \n",
      "Day 2: Île de la Cité (Notre-Dame, Sainte-Chapelle) → Latin Quarter → Musée d’Orsay → Saint-Germain cafés  \n",
      "Day 3: Montmartre & Sacré-Cœur → Le Marais (Place des Vosges, shopping) → Centre Pompidou → evening stroll along Canal Saint-Martin  \n",
      "\n",
      "Whatever you choose, leave time simply to wander—every street corner in Paris has its own little discovery. Enjoy your trip!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "endpoint = \"https://dalle-declare.openai.azure.com/\"\n",
    "model_name = \"o4-mini\"\n",
    "deployment = \"o4-mini\"\n",
    "\n",
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=os.getenv(\"AZURE_API_KEY\"),\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am going to Paris, what should I see?\",\n",
    "        }\n",
    "    ],\n",
    "    max_completion_tokens=100000,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAVENRunner import...\n",
      "RAVENRunner class imported successfully!\n",
      "Testing NEW combined image approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created prompt with <PIL.Image.Image image mode=RGB size=848x1102 at 0x793174A6A990> combined image(s) (should be 1)\n",
      "✓ Combined image size: (848, 1102)\n",
      "✓ Ground truth target: 4 (zero-indexed) = choice 5\n",
      "\n",
      "============================================================\n",
      "NEW COMBINED IMAGE PROMPT:\n",
      "============================================================\n",
      "You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your task is to identify which of the 8 numbered choices (1-8) best completes the pattern.\n",
      "\n",
      "Select the choice (1-8) that best completes the pattern. Respond with only the number (1, 2, 3, 4, 5, 6, 7, or 8).\n",
      "============================================================\n",
      "\n",
      "Displaying the combined image...\n",
      "✓ Saved combined image as 'test_combined_raven_image.png'\n",
      "\n",
      "Testing combined image encoding...\n",
      "✓ Successfully encoded combined image: 79404 chars\n",
      "✓ Encoded starts with: iVBORw0KGgoAAAANSUhEUgAAA1AAAA...\n",
      "\n",
      "Image dimensions:\n",
      "✓ Combined image: (848, 1102)\n",
      "✓ Width x Height: 848 x 1102 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "display-im6.q16: unable to open X server `' @ error/display.c/DisplayImageCommand/412.\n"
     ]
    }
   ],
   "source": [
    "# Test the RAVENRunner class\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from preprocessing.load_raven import RAVENRunner\n",
    "import os\n",
    "\n",
    "# Check if we can import and initialize (without real credentials)\n",
    "print(\"Testing RAVENRunner import...\")\n",
    "print(\"RAVENRunner class imported successfully!\")\n",
    "\n",
    "# Test the NEW combined image approach (single image instead of two)\n",
    "print(\"Testing NEW combined image approach...\")\n",
    "\n",
    "# Initialize runner with updated code\n",
    "runner = RAVENRunner(model_name=\"azure/o4-mini\", reasoning_effort=\"high\")\n",
    "\n",
    "example = runner.datasets['validation'][0]\n",
    "prompt, combined_image = runner.create_raven_prompt(example['panels'], example['choices'])\n",
    "\n",
    "print(f\"✓ Created prompt with {combined_image} combined image(s) (should be 1)\")\n",
    "print(f\"✓ Combined image size: {combined_image.size}\")\n",
    "print(f\"✓ Ground truth target: {example['target']} (zero-indexed) = choice {example['target'] + 1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEW COMBINED IMAGE PROMPT:\")\n",
    "print(\"=\"*60)\n",
    "print(prompt)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display and save the combined image for visual inspection\n",
    "print(f\"\\nDisplaying the combined image...\")\n",
    "\n",
    "# Display the combined image (this will open in your default image viewer)\n",
    "combined_image.show()\n",
    "\n",
    "# Save the combined image for inspection\n",
    "combined_image.save(\"test_combined_raven_image.png\")\n",
    "print(\"✓ Saved combined image as 'test_combined_raven_image.png'\")\n",
    "\n",
    "# Test image encoding for API readiness\n",
    "print(f\"\\nTesting combined image encoding...\")\n",
    "encoded_image = runner.encode_image_to_base64(combined_image)\n",
    "print(f\"✓ Successfully encoded combined image: {len(encoded_image)} chars\")\n",
    "print(f\"✓ Encoded starts with: {encoded_image[:30]}...\")\n",
    "\n",
    "# Show dimensions comparison\n",
    "print(f\"\\nImage dimensions:\")\n",
    "print(f\"✓ Combined image: {combined_image.size}\")\n",
    "print(f\"✓ Width x Height: {combined_image.size[0]} x {combined_image.size[1]} pixels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single API call with combined image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: \n",
      "WARNING:preprocessing.load_raven:Received empty response from API\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: \n",
      "WARNING:preprocessing.load_raven:Received empty response from API\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: \n",
      "WARNING:preprocessing.load_raven:Received empty response from API\n",
      "INFO:llm_logger:Added to cache\n",
      "WARNING:preprocessing.load_raven:Empty response received\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API call successful!\n",
      "✓ Raw response: ''\n",
      "✓ Parsed response: -1 (zero-indexed)\n",
      "✗ Invalid response - could not parse\n"
     ]
    }
   ],
   "source": [
    "# Test a single API call with the combined image\n",
    "print(\"Testing single API call with combined image...\")\n",
    "\n",
    "try:\n",
    "    # Make a single API call to test the new combined image approach\n",
    "    response = runner.query_azure_openai(prompt, combined_image)\n",
    "    \n",
    "    print(f\"✓ API call successful!\")\n",
    "    print(f\"✓ Raw response: '{response}'\")\n",
    "    \n",
    "    # Parse the response\n",
    "    parsed_response = runner.parse_response(response)\n",
    "    print(f\"✓ Parsed response: {parsed_response} (zero-indexed)\")\n",
    "    \n",
    "    if parsed_response != -1:\n",
    "        print(f\"✓ Model chose option: {parsed_response + 1} (1-indexed)\")\n",
    "        print(f\"✓ Ground truth: {example['target'] + 1} (1-indexed)\")\n",
    "        print(f\"✓ Correct: {parsed_response == example['target']}\")\n",
    "    else:\n",
    "        print(\"✗ Invalid response - could not parse\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ API call failed: {e}\")\n",
    "    print(\"This might be due to missing API credentials or network issues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n",
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n",
      "INFO:preprocessing.load_raven:Evaluating 5 samples from validation split...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:preprocessing.load_raven:Detailed results saved to raven_results_validation_5samples.json\n",
      "INFO:preprocessing.load_raven:Batch evaluating 10 samples (batch_size=3)...\n",
      "INFO:preprocessing.load_raven:Processing batch 1: samples 0-2\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 5 validation samples...\n",
      "Results:\n",
      "Accuracy: 0.200\n",
      "Correct: 1/5\n",
      "Invalid responses: 0\n",
      "Valid accuracy: 0.200\n",
      "\\nFor larger evaluations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:preprocessing.load_raven:Pausing between batches...\n",
      "INFO:preprocessing.load_raven:Processing batch 2: samples 3-5\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:preprocessing.load_raven:Pausing between batches...\n",
      "INFO:preprocessing.load_raven:Processing batch 3: samples 6-8\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "INFO:preprocessing.load_raven:Pausing between batches...\n",
      "INFO:preprocessing.load_raven:Processing batch 4: samples 9-9\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables for your chosen model:\n",
    "runner = RAVENRunner(model_name=\"azure/gpt-4.1\", reasoning_effort=\"high\")\n",
    "\n",
    "\n",
    "# Run small test evaluation\n",
    "print(\"Running evaluation on 5 validation samples...\")\n",
    "results = runner.evaluate_dataset(\"validation\", max_samples=5)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"Accuracy: {results['accuracy']:.3f}\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"Invalid responses: {results['invalid_responses']}\")\n",
    "print(f\"Valid accuracy: {results['valid_accuracy']:.3f}\")\n",
    "\n",
    "# For larger evaluations, use batch processing\n",
    "print(\"\\\\nFor larger evaluations:\")\n",
    "batch_results = runner.batch_evaluate(\"validation\", batch_size=3, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n",
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n",
      "INFO:preprocessing.load_raven:Evaluating 5 samples from validation split...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "WARNING:preprocessing.load_raven:Empty response received\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "WARNING:preprocessing.load_raven:Empty response received\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "WARNING:preprocessing.load_raven:Empty response received\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "WARNING:preprocessing.load_raven:Empty response received\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your tas...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: You are shown a 3x3 matrix of images with the bott...\n",
      "WARNING:preprocessing.load_raven:Empty response received\n",
      "INFO:preprocessing.load_raven:Detailed results saved to raven_results_validation_5samples.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 5 validation samples...\n",
      "Results:\n",
      "Accuracy: 0.000\n",
      "Correct: 0/5\n",
      "Invalid responses: 5\n",
      "Valid accuracy: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables for your chosen model:\n",
    "\n",
    "# Initialize runner\n",
    "runner = RAVENRunner(\n",
    "    model_name=\"azure/o_series/o4-mini\",  # or \"vertex_ai/claude-3-7-sonnet@20250219\"\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "\n",
    "# Run small test evaluation\n",
    "print(\"Running evaluation on 5 validation samples...\")\n",
    "results = runner.evaluate_dataset(\"validation\", max_samples=5)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"Accuracy: {results['accuracy']:.3f}\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"Invalid responses: {results['invalid_responses']}\")\n",
    "print(f\"Valid accuracy: {results['valid_accuracy']:.3f}\")\n",
    "\n",
    "# # For larger evaluations, use batch processing\n",
    "# print(\"\\\\nFor larger evaluations:\")\n",
    "# batch_results = runner.batch_evaluate(\"validation\", batch_size=3, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmr_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
