{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceM4/RAVEN\", \"center_single\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['panels', 'choices', 'structure', 'meta_matrix', 'meta_target', 'meta_structure', 'target', 'id', 'metadata'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['panels', 'choices', 'structure', 'meta_matrix', 'meta_target', 'meta_structure', 'target', 'id', 'metadata']\n",
      "\n",
      "Panels shape: 8\n",
      "Choices shape: 8\n",
      "Target: 4\n",
      "ID: 3023\n",
      "\n",
      "First panel type: <class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "First choice type: <class 'PIL.PngImagePlugin.PngImageFile'>\n"
     ]
    }
   ],
   "source": [
    "# Examine the structure of a single example\n",
    "example = ds[0]\n",
    "print(\"Keys:\", list(example.keys()))\n",
    "print(\"\\nPanels shape:\", len(example['panels']))\n",
    "print(\"Choices shape:\", len(example['choices']))\n",
    "print(\"Target:\", example['target'])\n",
    "print(\"ID:\", example['id'])\n",
    "\n",
    "# Look at the first panel and choice to understand the image format\n",
    "print(\"\\nFirst panel type:\", type(example['panels'][0]))\n",
    "print(\"First choice type:\", type(example['choices'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAVENRunner import...\n",
      "RAVENRunner class imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test the RAVENRunner class\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from preprocessing.load_raven import RAVENRunner\n",
    "import os\n",
    "\n",
    "# Check if we can import and initialize (without real credentials)\n",
    "print(\"Testing RAVENRunner import...\")\n",
    "print(\"RAVENRunner class imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAVENRunner initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAVENRunner initialized successfully!\n",
      "✓ Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n",
      "✓ Validation sample keys: ['panels', 'choices', 'structure', 'meta_matrix', 'meta_target', 'meta_structure', 'target', 'id', 'metadata']\n",
      "✓ Target: 4 (zero-indexed), ID: 5047\n",
      "✓ Target as 1-indexed choice: 5\n"
     ]
    }
   ],
   "source": [
    "# Test RAVENRunner initialization (won't actually call API)\n",
    "print(\"Testing RAVENRunner initialization...\")\n",
    "\n",
    "try:\n",
    "    # This will load the datasets but won't make API calls\n",
    "    runner = RAVENRunner(\n",
    "        model_name=\"azure/gpt-4.1\",\n",
    "        reasoning_effort=\"high\"\n",
    "    )\n",
    "    print(\"✓ RAVENRunner initialized successfully!\")\n",
    "    print(f\"✓ Loaded datasets: {[(k, len(v)) for k, v in runner.datasets.items()]}\")\n",
    "    \n",
    "    # Test dataset access\n",
    "    val_sample = runner.datasets['validation'][0]\n",
    "    print(f\"✓ Validation sample keys: {list(val_sample.keys())}\")\n",
    "    print(f\"✓ Target: {val_sample['target']} (zero-indexed), ID: {val_sample['id']}\")\n",
    "    print(f\"✓ Target as 1-indexed choice: {val_sample['target'] + 1}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pattern is that each column keeps the same shape, each row the same size (big, medium, small), and the three fill‐levels (white, light‐gray, dark‐gray) rotate “down” each column.  In column 3 (the triangles) you have:\n",
      "\n",
      " Row 1: light‐gray  \n",
      " Row 2: dark‐gray  \n",
      " Row 3: must be white  \n",
      "\n",
      "And in row 3 everything is the small version.  Therefore you need the small, unfilled (white) triangle – that is choice 6.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from mimetypes import guess_type\n",
    "\n",
    "# Function to encode a local image into data URL \n",
    "def local_image_to_data_url(image_path):\n",
    "    # Guess the MIME type of the image based on the file extension\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'  # Default MIME type if none is found\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n",
    "\n",
    "# Example usage\n",
    "image_path = 'test_combined_raven_image.png'\n",
    "data_url = local_image_to_data_url(image_path)\n",
    "# print(\"Data URL:\", data_url)\n",
    "\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "endpoint = \"https://dalle-declare.openai.azure.com/\"\n",
    "model_name = \"o4-mini\"\n",
    "deployment = \"o4-mini\"\n",
    "\n",
    "api_version = \"2025-01-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=os.getenv(\"AZURE_API_KEY\"),\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What is the answer?\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": data_url}}\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    max_completion_tokens=100000,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAVENRunner import...\n",
      "RAVENRunner class imported successfully!\n",
      "Testing NEW combined image approach...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ace6561359417cbaa11921540d259a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/163M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27384e8261864e4ca7a17e132b101236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/54.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b53532bcae44577b233c5e4ece9d1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/54.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c49a726acb149d8a959126e4b7c9cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711fd8fe72bd4a139af224507b47ca33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f250cff41cc4763814a3198bbaab75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created prompt with <PIL.Image.Image image mode=RGB size=848x1102 at 0x7B95E2E70860> combined image(s) (should be 1)\n",
      "✓ Combined image size: (848, 1102)\n",
      "✓ Ground truth target: 0 (zero-indexed) = choice 1\n",
      "\n",
      "============================================================\n",
      "NEW COMBINED IMAGE PROMPT:\n",
      "============================================================\n",
      "You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). Your task is to identify which of the 8 numbered choices (1-8) in the Answer Set best completes the pattern.\n",
      "\n",
      "Select the choice (1-8) that best completes the pattern. Respond with only the number (1, 2, 3, 4, 5, 6, 7, or 8).\n",
      "============================================================\n",
      "\n",
      "Displaying the combined image...\n",
      "✓ Saved combined image as 'test_combined_raven_image.png'\n",
      "\n",
      "Testing combined image encoding...\n",
      "✓ Successfully encoded combined image: 49964 chars\n",
      "✓ Encoded starts with: iVBORw0KGgoAAAANSUhEUgAAA1AAAA...\n",
      "\n",
      "Image dimensions:\n",
      "✓ Combined image: (848, 1102)\n",
      "✓ Width x Height: 848 x 1102 pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "display-im6.q16: unable to open X server `' @ error/display.c/DisplayImageCommand/412.\n"
     ]
    }
   ],
   "source": [
    "# Test the RAVENRunner class\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from preprocessing.load_raven import RAVENRunner\n",
    "import os\n",
    "\n",
    "# Check if we can import and initialize (without real credentials)\n",
    "print(\"Testing RAVENRunner import...\")\n",
    "print(\"RAVENRunner class imported successfully!\")\n",
    "\n",
    "# Test the NEW combined image approach (single image instead of two)\n",
    "print(\"Testing NEW combined image approach...\")\n",
    "\n",
    "# Initialize runner with updated code\n",
    "runner = RAVENRunner(model_name=\"azure/o4-mini\", reasoning_effort=\"high\")\n",
    "\n",
    "example = runner.datasets['validation'][0]\n",
    "prompt, combined_image = runner.create_raven_prompt(example['panels'], example['choices'])\n",
    "\n",
    "print(f\"✓ Created prompt with {combined_image} combined image(s) (should be 1)\")\n",
    "print(f\"✓ Combined image size: {combined_image.size}\")\n",
    "print(f\"✓ Ground truth target: {example['target']} (zero-indexed) = choice {example['target'] + 1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEW COMBINED IMAGE PROMPT:\")\n",
    "print(\"=\"*60)\n",
    "print(prompt)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display and save the combined image for visual inspection\n",
    "print(f\"\\nDisplaying the combined image...\")\n",
    "\n",
    "# Display the combined image (this will open in your default image viewer)\n",
    "combined_image.show()\n",
    "\n",
    "# Save the combined image for inspection\n",
    "combined_image.save(\"test_combined_raven_image.png\")\n",
    "print(\"✓ Saved combined image as 'test_combined_raven_image.png'\")\n",
    "\n",
    "# Test image encoding for API readiness\n",
    "print(f\"\\nTesting combined image encoding...\")\n",
    "encoded_image = runner.encode_image_to_base64(combined_image)\n",
    "print(f\"✓ Successfully encoded combined image: {len(encoded_image)} chars\")\n",
    "print(f\"✓ Encoded starts with: {encoded_image[:30]}...\")\n",
    "\n",
    "# Show dimensions comparison\n",
    "print(f\"\\nImage dimensions:\")\n",
    "print(f\"✓ Combined image: {combined_image.size}\")\n",
    "print(f\"✓ Width x Height: {combined_image.size[0]} x {combined_image.size[1]} pixels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single API call with combined image...\n",
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API call successful!\n",
      "✓ Raw response: '1'\n",
      "✓ Parsed response: 0 (zero-indexed)\n",
      "✓ Model chose option: 1 (1-indexed)\n",
      "✓ Ground truth: 1 (1-indexed)\n",
      "✓ Correct: True\n"
     ]
    }
   ],
   "source": [
    "# Test a single API call with the combined image\n",
    "print(\"Testing single API call with combined image...\")\n",
    "\n",
    "try:\n",
    "    # Make a single API call to test the new combined image approach\n",
    "    response = runner.query_azure_openai(prompt, combined_image)\n",
    "    \n",
    "    print(f\"✓ API call successful!\")\n",
    "    print(f\"✓ Raw response: '{response}'\")\n",
    "    \n",
    "    # Parse the response\n",
    "    parsed_response = runner.parse_response(response)\n",
    "    print(f\"✓ Parsed response: {parsed_response} (zero-indexed)\")\n",
    "    \n",
    "    if parsed_response != -1:\n",
    "        print(f\"✓ Model chose option: {parsed_response + 1} (1-indexed)\")\n",
    "        print(f\"✓ Ground truth: {example['target'] + 1} (1-indexed)\")\n",
    "        print(f\"✓ Correct: {parsed_response == example['target']}\")\n",
    "    else:\n",
    "        print(\"✗ Invalid response - could not parse\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ API call failed: {e}\")\n",
    "    print(\"This might be due to missing API credentials or network issues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n",
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n",
      "INFO:preprocessing.load_raven:Evaluating 20 samples from validation split...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:08:58 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 5 validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:00 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:00 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 8\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:00 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:02 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:02 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 1\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:02 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:04 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:04 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 8\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:04 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:05 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:05 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 6\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:05 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 7\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:07 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:08 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:08 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 8\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:08 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 4\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:10 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:11 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:11 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 1\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:11 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:13 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:13 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 4\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:13 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:15 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:15 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 1\n",
      "INFO:preprocessing.load_raven:Processed 10/20 samples\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:15 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:16 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 7\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:16 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 7\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:18 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:19 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 8\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:19 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:19 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 8\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:20 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:22 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:22 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 5\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:22 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 8\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:24 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:25 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:25 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 5\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:25 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:27 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 6\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:27 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:28 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 7\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m14:09:28 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m14:09:30 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m14:09:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: 6\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:preprocessing.load_raven:Processed 20/20 samples\n",
      "INFO:preprocessing.load_raven:Detailed results saved to raven_results_validation_20samples.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Accuracy: 0.100\n",
      "Correct: 2/20\n",
      "Invalid responses: 0\n",
      "Valid accuracy: 0.100\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "from preprocessing.load_raven import RAVENRunner\n",
    "\n",
    "# Set environment variables for your chosen model:\n",
    "runner = RAVENRunner(model_name=\"azure/gpt-4.1\", reasoning_effort=\"high\")\n",
    "\n",
    "\n",
    "# Run small test evaluation\n",
    "print(\"Running evaluation on 5 validation samples...\")\n",
    "results = runner.evaluate_dataset(\"validation\", max_samples=20)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"Accuracy: {results['accuracy']:.3f}\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"Invalid responses: {results['invalid_responses']}\")\n",
    "print(f\"Valid accuracy: {results['valid_accuracy']:.3f}\")\n",
    "\n",
    "# # For larger evaluations, use batch processing\n",
    "# print(\"\\\\nFor larger evaluations:\")\n",
    "# batch_results = runner.batch_evaluate(\"validation\", batch_size=3, max_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n",
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n",
      "INFO:preprocessing.load_raven:Evaluating 10 samples from validation split...\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 5 validation samples...\n",
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 6\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 7\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 2\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 6\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 3\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 1\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 7\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 8\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 5\n",
      "INFO:llm_logger:PROMPT: You are shown a 3x3 Problem Matrix of images with the bottom-right panel missing (marked with \"?\"). ...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AzureOpenAI client for o4 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "INFO:llm_logger:RESPONSE: 4\n",
      "INFO:preprocessing.load_raven:Processed 10/10 samples\n",
      "INFO:preprocessing.load_raven:Detailed results saved to raven_results_validation_10samples.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Accuracy: 0.400\n",
      "Correct: 4/10\n",
      "Invalid responses: 0\n",
      "Valid accuracy: 0.400\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "from preprocessing.load_raven import RAVENRunner\n",
    "\n",
    "# Initialize runner\n",
    "runner = RAVENRunner(\n",
    "    model_name=\"azure/o4-mini\",  # or \"vertex_ai/claude-3-7-sonnet@20250219\"\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "\n",
    "# Run small test evaluation\n",
    "print(\"Running evaluation on 5 validation samples...\")\n",
    "results = runner.evaluate_dataset(\"validation\", max_samples=10)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"Accuracy: {results['accuracy']:.3f}\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"Invalid responses: {results['invalid_responses']}\")\n",
    "print(f\"Valid accuracy: {results['valid_accuracy']:.3f}\")\n",
    "\n",
    "# # For larger evaluations, use batch processing\n",
    "# print(\"\\\\nFor larger evaluations:\")\n",
    "# batch_results = runner.batch_evaluate(\"validation\", batch_size=3, max_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmr_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
