{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"HuggingFaceM4/RAVEN\", \"center_single\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['panels', 'choices', 'structure', 'meta_matrix', 'meta_target', 'meta_structure', 'target', 'id', 'metadata'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['panels', 'choices', 'structure', 'meta_matrix', 'meta_target', 'meta_structure', 'target', 'id', 'metadata']\n",
      "\n",
      "Panels shape: 8\n",
      "Choices shape: 8\n",
      "Target: 4\n",
      "ID: 3023\n",
      "\n",
      "First panel type: <class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "First choice type: <class 'PIL.PngImagePlugin.PngImageFile'>\n"
     ]
    }
   ],
   "source": [
    "# Examine the structure of a single example\n",
    "example = ds[0]\n",
    "print(\"Keys:\", list(example.keys()))\n",
    "print(\"\\nPanels shape:\", len(example['panels']))\n",
    "print(\"Choices shape:\", len(example['choices']))\n",
    "print(\"Target:\", example['target'])\n",
    "print(\"ID:\", example['id'])\n",
    "\n",
    "# Look at the first panel and choice to understand the image format\n",
    "print(\"\\nFirst panel type:\", type(example['panels'][0]))\n",
    "print(\"First choice type:\", type(example['choices'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAVENRunner import...\n",
      "RAVENRunner class imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test the RAVENRunner class\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from preprocessing.load_raven import RAVENRunner\n",
    "import os\n",
    "\n",
    "# Check if we can import and initialize (without real credentials)\n",
    "print(\"Testing RAVENRunner import...\")\n",
    "print(\"RAVENRunner class imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAVENRunner initialization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAVENRunner initialized successfully!\n",
      "✓ Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n",
      "✓ Validation sample keys: ['panels', 'choices', 'structure', 'meta_matrix', 'meta_target', 'meta_structure', 'target', 'id', 'metadata']\n",
      "✓ Target: 4 (zero-indexed), ID: 5047\n",
      "✓ Target as 1-indexed choice: 5\n"
     ]
    }
   ],
   "source": [
    "# Test RAVENRunner initialization (won't actually call API)\n",
    "print(\"Testing RAVENRunner initialization...\")\n",
    "\n",
    "try:\n",
    "    # This will load the datasets but won't make API calls\n",
    "    runner = RAVENRunner(\n",
    "        model_name=\"azure/gpt-4.1\",\n",
    "        reasoning_effort=\"high\"\n",
    "    )\n",
    "    print(\"✓ RAVENRunner initialized successfully!\")\n",
    "    print(f\"✓ Loaded datasets: {[(k, len(v)) for k, v in runner.datasets.items()]}\")\n",
    "    \n",
    "    # Test dataset access\n",
    "    val_sample = runner.datasets['validation'][0]\n",
    "    print(f\"✓ Validation sample keys: {list(val_sample.keys())}\")\n",
    "    print(f\"✓ Target: {val_sample['target']} (zero-indexed), ID: {val_sample['id']}\")\n",
    "    print(f\"✓ Target as 1-indexed choice: {val_sample['target'] + 1}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n",
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n",
      "INFO:preprocessing.load_raven:Evaluating 5 samples from validation split...\n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "\u001b[92m11:21:18 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= o4-mini; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= o4-mini; provider = azure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on 5 validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/o4-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m11:21:20 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:21:20 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/o4-mini-2025-04-16\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/o4-mini-2025-04-16\n",
      "INFO:llm_logger:RESPONSE: \n",
      "INFO:llm_logger:Added to cache\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:preprocessing.load_raven:Detailed results saved to raven_results_validation_5samples.json\n",
      "INFO:preprocessing.load_raven:Batch evaluating 10 samples (batch_size=3)...\n",
      "INFO:preprocessing.load_raven:Processing batch 1: samples 0-2\n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:preprocessing.load_raven:Pausing between batches...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Accuracy: 0.000\n",
      "Correct: 0/5\n",
      "Invalid responses: 5\n",
      "Valid accuracy: 0.000\n",
      "\\nFor larger evaluations:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Processing batch 2: samples 3-5\n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:preprocessing.load_raven:Pausing between batches...\n",
      "INFO:preprocessing.load_raven:Processing batch 3: samples 6-8\n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n",
      "INFO:preprocessing.load_raven:Pausing between batches...\n",
      "INFO:preprocessing.load_raven:Processing batch 4: samples 9-9\n",
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/o4-mini, REASONING: high\n",
      "INFO:llm_logger:Cache hit for prompt: This is a visual reasoning task. You are shown a 3...\n",
      "WARNING:preprocessing.load_raven:Invalid response format: \n"
     ]
    }
   ],
   "source": [
    "# Set environment variables for your chosen model:\n",
    "\n",
    "# Initialize runner\n",
    "runner = RAVENRunner(\n",
    "    model_name=\"azure/gpt4.1\",  # or \"vertex_ai/claude-3-7-sonnet@20250219\"\n",
    "    reasoning_effort=\"high\"\n",
    ")\n",
    "\n",
    "# Run small test evaluation\n",
    "print(\"Running evaluation on 5 validation samples...\")\n",
    "results = runner.evaluate_dataset(\"validation\", max_samples=5)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"Accuracy: {results['accuracy']:.3f}\")\n",
    "print(f\"Correct: {results['correct']}/{results['total']}\")\n",
    "print(f\"Invalid responses: {results['invalid_responses']}\")\n",
    "print(f\"Valid accuracy: {results['valid_accuracy']:.3f}\")\n",
    "\n",
    "# For larger evaluations, use batch processing\n",
    "print(\"\\\\nFor larger evaluations:\")\n",
    "batch_results = runner.batch_evaluate(\"validation\", batch_size=3, max_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loading RAVEN dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAVENRunner import...\n",
      "RAVENRunner class imported successfully!\n",
      "Testing NEW combined image approach...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:preprocessing.load_raven:Loaded datasets: [('train', 6000), ('validation', 2000), ('test', 2000)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created prompt with 1 combined image(s) (should be 1)\n",
      "✓ Combined image size: (818, 1102)\n",
      "✓ Ground truth target: 4 (zero-indexed) = choice 5\n",
      "\n",
      "============================================================\n",
      "NEW COMBINED IMAGE PROMPT:\n",
      "============================================================\n",
      "This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel missing (marked with \"?\"). Your task is to identify which of the 8 numbered choices (1-8) best completes the pattern.\n",
      "\n",
      "The image shows:\n",
      "- TOP: A 3x3 matrix with 8 panels arranged as:\n",
      "  [Panel 1] [Panel 2] [Panel 3]\n",
      "  [Panel 4] [Panel 5] [Panel 6] \n",
      "  [Panel 7] [Panel 8] [   ?   ]\n",
      "\n",
      "- BOTTOM: 8 numbered choices (1 through 8) that could complete the matrix.\n",
      "\n",
      "TASK:\n",
      "Analyze the patterns in the matrix:\n",
      "- Look for horizontal patterns (left to right across rows)\n",
      "- Look for vertical patterns (top to bottom down columns)  \n",
      "- Look for diagonal patterns\n",
      "- Consider transformations like rotation, reflection, addition/removal of elements\n",
      "- Consider relationships between shapes, colors, positions, and quantities\n",
      "\n",
      "Select the choice (1-8) that best completes the pattern. Respond with only the number (1, 2, 3, 4, 5, 6, 7, or 8).\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the RAVENRunner class\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from preprocessing.load_raven import RAVENRunner\n",
    "import os\n",
    "\n",
    "# Check if we can import and initialize (without real credentials)\n",
    "print(\"Testing RAVENRunner import...\")\n",
    "print(\"RAVENRunner class imported successfully!\")\n",
    "\n",
    "# Test the NEW combined image approach (single image instead of two)\n",
    "print(\"Testing NEW combined image approach...\")\n",
    "\n",
    "# Initialize runner with updated code\n",
    "runner = RAVENRunner(model_name=\"azure/gpt-4.1\", reasoning_effort=\"high\")\n",
    "\n",
    "example = runner.datasets['validation'][0]\n",
    "prompt, combined_images = runner.create_raven_prompt(example['panels'], example['choices'])\n",
    "\n",
    "print(f\"✓ Created prompt with {len(combined_images)} combined image(s) (should be 1)\")\n",
    "print(f\"✓ Combined image size: {combined_images[0].size}\")\n",
    "print(f\"✓ Ground truth target: {example['target']} (zero-indexed) = choice {example['target'] + 1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEW COMBINED IMAGE PROMPT:\")\n",
    "print(\"=\"*60)\n",
    "print(prompt)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display and save the combined image for visual inspection\n",
    "print(f\"\\nDisplaying the combined image...\")\n",
    "\n",
    "# Display the combined image (this will open in your default image viewer)\n",
    "combined_images[0].show()\n",
    "\n",
    "# Save the combined image for inspection\n",
    "combined_images[0].save(\"test_combined_raven_image.png\")\n",
    "print(\"✓ Saved combined image as 'test_combined_raven_image.png'\")\n",
    "\n",
    "# Test image encoding for API readiness\n",
    "print(f\"\\nTesting combined image encoding...\")\n",
    "encoded_image = runner.encode_image_to_base64(combined_images[0])\n",
    "print(f\"✓ Successfully encoded combined image: {len(encoded_image)} chars\")\n",
    "print(f\"✓ Encoded starts with: {encoded_image[:30]}...\")\n",
    "\n",
    "# Show dimensions comparison\n",
    "print(f\"\\nImage dimensions:\")\n",
    "print(f\"✓ Combined image: {combined_images[0].size}\")\n",
    "print(f\"✓ Width x Height: {combined_images[0].size[0]} x {combined_images[0].size[1]} pixels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm_logger:PROMPT: This is a visual reasoning task. You are shown a 3x3 matrix of images with the bottom-right panel mi...\n",
      "INFO:llm_logger:MODEL: azure/gpt-4.1, REASONING: high\n",
      "\u001b[92m11:35:11 - LiteLLM:INFO\u001b[0m: utils.py:3043 - \n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= gpt-4.1; provider = azure\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing single API call with combined image...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://dalle-declare.openai.azure.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m11:35:14 - LiteLLM:INFO\u001b[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m11:35:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m11:35:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "\u001b[92m11:35:14 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:LiteLLM:selected model name for cost calculation: azure/gpt-4.1-2025-04-14\n",
      "INFO:llm_logger:RESPONSE: The answer is **7**.\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "- Each row has a shape: circles in the first column, pentagons in the second, and triangles in the third.\n",
      "- Each row’s shape decreases in size from top to bottom,\n",
      "INFO:llm_logger:Added to cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API call successful!\n",
      "✓ Raw response: 'The answer is **7**.\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "- Each row has a shape: circles in the first column, pentagons in the second, and triangles in the third.\n",
      "- Each row’s shape decreases in size from top to bottom,'\n",
      "✓ Parsed response: 6 (zero-indexed)\n",
      "✓ Model chose option: 7 (1-indexed)\n",
      "✓ Ground truth: 5 (1-indexed)\n",
      "✓ Correct: False\n"
     ]
    }
   ],
   "source": [
    "# Test a single API call with the combined image\n",
    "print(\"Testing single API call with combined image...\")\n",
    "\n",
    "try:\n",
    "    # Make a single API call to test the new combined image approach\n",
    "    response = runner.query_azure_openai(prompt, combined_images)\n",
    "    \n",
    "    print(f\"✓ API call successful!\")\n",
    "    print(f\"✓ Raw response: '{response}'\")\n",
    "    \n",
    "    # Parse the response\n",
    "    parsed_response = runner.parse_response(response)\n",
    "    print(f\"✓ Parsed response: {parsed_response} (zero-indexed)\")\n",
    "    \n",
    "    if parsed_response != -1:\n",
    "        print(f\"✓ Model chose option: {parsed_response + 1} (1-indexed)\")\n",
    "        print(f\"✓ Ground truth: {example['target'] + 1} (1-indexed)\")\n",
    "        print(f\"✓ Correct: {parsed_response == example['target']}\")\n",
    "    else:\n",
    "        print(\"✗ Invalid response - could not parse\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ API call failed: {e}\")\n",
    "    print(\"This might be due to missing API credentials or network issues\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare old vs new approach (for debugging)\n",
    "print(\"Comparing old vs new image approach...\")\n",
    "\n",
    "# Create individual composites (old way)\n",
    "matrix_composite = runner.create_composite_matrix_image(example['panels'])\n",
    "choices_composite = runner.create_choices_grid(example['choices'])\n",
    "\n",
    "# Create combined image (new way)\n",
    "combined_image = runner.create_combined_image(matrix_composite, choices_composite)\n",
    "\n",
    "print(f\"Old approach:\")\n",
    "print(f\"  ✓ Matrix composite: {matrix_composite.size}\")\n",
    "print(f\"  ✓ Choices composite: {choices_composite.size}\")\n",
    "print(f\"  ✓ Total images: 2\")\n",
    "\n",
    "print(f\"\\nNew approach:\")\n",
    "print(f\"  ✓ Combined image: {combined_image.size}\")\n",
    "print(f\"  ✓ Total images: 1\")\n",
    "\n",
    "print(f\"\\nSize comparison:\")\n",
    "print(f\"  ✓ Matrix height: {matrix_composite.size[1]}\")\n",
    "print(f\"  ✓ Choices height: {choices_composite.size[1]}\")\n",
    "print(f\"  ✓ Combined height: {combined_image.size[1]} (should be matrix + choices + padding)\")\n",
    "print(f\"  ✓ Expected height: {matrix_composite.size[1] + choices_composite.size[1] + 20}\")\n",
    "\n",
    "# Save individual components for comparison\n",
    "matrix_composite.save(\"test_matrix_only.png\")\n",
    "choices_composite.save(\"test_choices_only.png\")\n",
    "print(f\"\\n✓ Saved individual components: test_matrix_only.png, test_choices_only.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmr_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
